{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT as given in assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting point was to take the model that we were given in the assignment and see how it performs. We were given the predictions from the first 3 CV folds so I trained the next 7 and saved all 10 .txt files. The next code blocks will read in the 10 files and get the useful performance metrics from them. Accuracy, Precision, Recall, F1. They also store a list of what reviews the model has classifed incorrectly which will come in useful later on.\n",
    "\n",
    "For this baseline mode, there are 6 epochs per CV fold and a max sequence length of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_256_files = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    bert_256_files.append(f'256_BERT/{i}_pred.txt')\n",
    "    \n",
    "c_names = ['gold','pred','correct','text']\n",
    "\n",
    "df1 = pd.DataFrame(columns=c_names)\n",
    "df2 = pd.DataFrame(columns=c_names)\n",
    "df3 = pd.DataFrame(columns=c_names)\n",
    "df4 = pd.DataFrame(columns=c_names)\n",
    "df5 = pd.DataFrame(columns=c_names)\n",
    "df6 = pd.DataFrame(columns=c_names)\n",
    "df7 = pd.DataFrame(columns=c_names)\n",
    "df8 = pd.DataFrame(columns=c_names)\n",
    "df9 = pd.DataFrame(columns=c_names)\n",
    "df10 = pd.DataFrame(columns=c_names)\n",
    "\n",
    "dataframes_256 = [df1,df2,df3,df4,df5,df6,df7,df8,df9,df10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfs(files, df_list):\n",
    "    j = 0\n",
    "    for dataframe in df_list:\n",
    "\n",
    "        #dataframe = pd.DataFrame(columns=['index','gold','pred','correct','text'])\n",
    "        processed_lines = []\n",
    "\n",
    "        with open(files[j], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            count = 0\n",
    "            for line in lines[1:]:\n",
    "                tokens = line.split()\n",
    "                line_length = len(tokens)\n",
    "                temp_line = ''\n",
    "\n",
    "                for i in range(4, (line_length)):\n",
    "                    temp_line = temp_line + tokens[i] + ' '\n",
    "\n",
    "                processed_line = [tokens[1],tokens[2],tokens[3], temp_line]\n",
    "                processed_lines.append(processed_line)\n",
    "                dataframe.loc[count] = processed_line\n",
    "                count+=1\n",
    "        j+=1\n",
    "    return(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_256 = create_dfs(bert_256_files, dataframes_256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(dataframe):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    corrects = 0\n",
    "    errors = []\n",
    "    for i in range(0,len(dataframe)):\n",
    "        if dataframe.iat[i,2] == 'yes':\n",
    "            corrects += 1\n",
    "        else:\n",
    "            errors.append(i)\n",
    "        if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "            true_pos += 1\n",
    "        elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "            false_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "            true_neg += 1\n",
    "        elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "            false_pos += 1\n",
    "    \n",
    "    accuracy = corrects/len(dataframe)\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    f1_score = 2*((precision*recall)/(precision + recall))\n",
    "    return(accuracy,precision,recall,f1_score,errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averages(df_list):\n",
    "    accuracies = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    f1s = []\n",
    "    errors_list = []\n",
    "    for dataframe in df_list:    \n",
    "        true_pos = 0\n",
    "        true_neg = 0\n",
    "        false_pos = 0\n",
    "        false_neg = 0\n",
    "        corrects = 0\n",
    "        errors = []\n",
    "        for i in range(0,len(dataframe)):\n",
    "            if dataframe.iat[i,2] == 'yes':\n",
    "                corrects += 1\n",
    "            else:\n",
    "                errors.append(i)\n",
    "            if (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'pos'):\n",
    "                true_pos += 1\n",
    "            elif (dataframe.iat[i,0] == 'pos' and dataframe.iat[i,1] == 'neg'):\n",
    "                false_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'neg'):\n",
    "                true_neg += 1\n",
    "            elif (dataframe.iat[i,0] == 'neg' and dataframe.iat[i,1] == 'pos'):\n",
    "                false_pos += 1\n",
    "\n",
    "        accuracy = corrects/len(dataframe)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        precision = true_pos/(true_pos + false_pos)\n",
    "        precs.append(precision)\n",
    "\n",
    "        recall = true_pos/(true_pos + false_neg)\n",
    "        recs.append(recall)\n",
    "        \n",
    "        f1_score = 2*((precision*recall)/(precision + recall))\n",
    "        f1s.append(f1_score)\n",
    "        \n",
    "        errors_list.append(errors)\n",
    "        \n",
    "    return(sum(accuracies)/len(df_list),sum(precs)/len(df_list),sum(recs)/len(df_list),sum(f1s)/len(df_list), errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_averages_get_errors(dataframes, errorlist = False):\n",
    "    acc,prec,rec,f1,errors = get_averages(dataframes)\n",
    "    if errorlist == True:\n",
    "        return(errors)\n",
    "    else:\n",
    "        for i, dataframe in enumerate(dataframes):\n",
    "            scores = get_f1(dataframe)\n",
    "            print(f'Cross validation {i+1}')\n",
    "            print(f'The accuracy is {scores[0]*100:.2f}%')\n",
    "            print(f'The precision is {scores[1]*100:.2f}%')\n",
    "            print(f'The recall is {scores[2]*100:.2f}%')\n",
    "            print(f'The F1 score is {scores[3]*100:.2f}%')\n",
    "            print(f'The model got the following rows wrong {scores[4]}\\n')\n",
    "\n",
    "        print(f'The average accuracy is {acc*100:.2f}%')\n",
    "        print(f'The average precision is {prec*100:.2f}%')\n",
    "        print(f'The average recall is {rec*100:.2f}%')\n",
    "        print(f'The average F1 score is {f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation 1\n",
      "The accuracy is 90.00%\n",
      "The precision is 87.04%\n",
      "The recall is 94.00%\n",
      "The F1 score is 90.38%\n",
      "The model got the following rows wrong [1, 10, 44, 50, 82, 91, 108, 113, 117, 118, 124, 128, 129, 134, 135, 157, 171, 177, 197, 198]\n",
      "\n",
      "Cross validation 2\n",
      "The accuracy is 87.50%\n",
      "The precision is 85.71%\n",
      "The recall is 90.00%\n",
      "The F1 score is 87.80%\n",
      "The model got the following rows wrong [9, 14, 18, 50, 59, 62, 65, 78, 93, 94, 104, 105, 115, 118, 125, 137, 140, 142, 143, 146, 162, 167, 170, 177, 189]\n",
      "\n",
      "Cross validation 3\n",
      "The accuracy is 95.00%\n",
      "The precision is 93.27%\n",
      "The recall is 97.00%\n",
      "The F1 score is 95.10%\n",
      "The model got the following rows wrong [8, 44, 99, 100, 133, 142, 156, 162, 178, 196]\n",
      "\n",
      "Cross validation 4\n",
      "The accuracy is 91.00%\n",
      "The precision is 90.20%\n",
      "The recall is 92.00%\n",
      "The F1 score is 91.09%\n",
      "The model got the following rows wrong [14, 31, 36, 54, 83, 92, 94, 98, 105, 108, 109, 120, 153, 159, 161, 168, 181, 185]\n",
      "\n",
      "Cross validation 5\n",
      "The accuracy is 85.00%\n",
      "The precision is 88.04%\n",
      "The recall is 81.00%\n",
      "The F1 score is 84.38%\n",
      "The model got the following rows wrong [0, 5, 18, 19, 20, 26, 30, 32, 35, 36, 51, 55, 60, 64, 67, 91, 93, 95, 98, 109, 144, 155, 160, 167, 172, 186, 189, 191, 192, 199]\n",
      "\n",
      "Cross validation 6\n",
      "The accuracy is 89.00%\n",
      "The precision is 93.33%\n",
      "The recall is 84.00%\n",
      "The F1 score is 88.42%\n",
      "The model got the following rows wrong [1, 2, 7, 19, 32, 34, 39, 40, 41, 45, 63, 79, 80, 85, 90, 99, 102, 119, 147, 151, 158, 171]\n",
      "\n",
      "Cross validation 7\n",
      "The accuracy is 85.50%\n",
      "The precision is 83.81%\n",
      "The recall is 88.00%\n",
      "The F1 score is 85.85%\n",
      "The model got the following rows wrong [3, 6, 27, 32, 34, 43, 44, 55, 60, 65, 93, 99, 100, 104, 109, 117, 120, 122, 126, 135, 137, 145, 161, 166, 171, 182, 185, 187, 197]\n",
      "\n",
      "Cross validation 8\n",
      "The accuracy is 89.00%\n",
      "The precision is 90.62%\n",
      "The recall is 87.00%\n",
      "The F1 score is 88.78%\n",
      "The model got the following rows wrong [12, 17, 19, 21, 35, 40, 58, 61, 64, 81, 89, 90, 97, 109, 112, 117, 135, 142, 166, 172, 187, 191]\n",
      "\n",
      "Cross validation 9\n",
      "The accuracy is 88.50%\n",
      "The precision is 88.89%\n",
      "The recall is 88.00%\n",
      "The F1 score is 88.44%\n",
      "The model got the following rows wrong [16, 25, 42, 50, 52, 55, 56, 70, 74, 76, 82, 84, 101, 103, 126, 132, 144, 145, 151, 155, 176, 178, 190]\n",
      "\n",
      "Cross validation 10\n",
      "The accuracy is 92.50%\n",
      "The precision is 88.99%\n",
      "The recall is 97.00%\n",
      "The F1 score is 92.82%\n",
      "The model got the following rows wrong [47, 50, 63, 112, 117, 124, 128, 133, 167, 171, 186, 188, 190, 192, 195]\n",
      "\n",
      "The average accuracy is 89.30%\n",
      "The average precision is 88.99%\n",
      "The average recall is 89.80%\n",
      "The average F1 score is 89.31%\n"
     ]
    }
   ],
   "source": [
    "print_averages_get_errors(dataframes_256, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's already clear at this point that the baseline BERT model that we were provided with is an excellent classifier. The average accuracy is just under 90% right out of the box. The next step I wanted to take was to increase the max sequence length from 256 to 512, which I did in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}